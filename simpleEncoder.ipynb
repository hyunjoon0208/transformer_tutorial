{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 메소드로 만들어도 됨.\n",
    "def MultiHeadAttention(self, Q, K, V):\n",
    "    # Q, K, V: [batch_size, n_heads, seq_len, d_k]\n",
    "    num_batch, num_head, num_token_length, att_dim = K.shape\n",
    "\n",
    "    Q /=(att_dim**0.5)\n",
    "    attention_score = Q@ K.permute(0,1,3,2) # [num_batch, num_head, num_token_length, num_token_length]\n",
    "\n",
    "    attention_score = torch.softmax(attention_score, dim=3)\n",
    "\n",
    "    Z = attention_score @ V # [num_batch, num_head, num_token_length, att_dim]\n",
    "\n",
    "    return Z, attention_score\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __ini__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Q, K, V: [batch_size, n_heads, seq_len, d_k]\n",
    "        num_batch, num_head, num_token_length, att_dim = K.shape\n",
    "\n",
    "        Q /=(att_dim**0.5)\n",
    "        attention_score = Q@ K.permute(0,1,3,2) # [num_batch, num_head, num_token_length, num_token_length]\n",
    "\n",
    "        attention_score = torch.softmax(attention_score, dim=3)\n",
    "\n",
    "        Z = attention_score @ V # [num_batch, num_head, num_token_length, att_dim]\n",
    "\n",
    "        return Z, attention_score\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
