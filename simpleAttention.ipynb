{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention Block\n",
    "- MultiHead Attention\n",
    "- Encoder Layer\n",
    "- Pytorch Official Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(torch.randn(8,3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5045,  1.0519,  0.0865],\n",
      "        [ 0.2906,  0.0111, -0.0254],\n",
      "        [ 1.4568,  0.1470, -0.0424],\n",
      "        [ 0.2881, -0.2398, -1.5240],\n",
      "        [ 0.4218,  0.5200,  0.0203],\n",
      "        [-1.4034, -0.1837,  1.1705],\n",
      "        [-0.9750, -1.1585,  0.9313],\n",
      "        [-1.6690, -0.2478,  1.4515]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_Q = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "w_K = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "w_V = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.matmul(X, w_Q)\n",
    "K = torch.matmul(X, w_K)\n",
    "V = torch.matmul(X, w_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul하는 다른 방법\n",
    "# torch.mm(X, w_Q)\n",
    "# X.mm(w_Q)\n",
    "# X.matmul(w_Q)\n",
    "# X @ w_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# query - key matching\n",
    "d_k = K.shape[1]\n",
    "print(d_k)\n",
    "# attention_score = Q@K.t() 원래는 이렇게\n",
    "# attention_score = attention_score / (d_k ** 0.5)\n",
    "# 계산량을 줄이기 위해\n",
    "attention_score = (Q / (d_k ** 0.5))@K.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.softmax(attention_score, dim=1) class로 구현된 softmax\n",
    "attention_score = torch.softmax(attention_score, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "Z = attention_score@V\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda12.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
